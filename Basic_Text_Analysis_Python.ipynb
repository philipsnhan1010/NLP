{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philipsnhan1010/NLP/blob/main/Basic_Text_Analysis_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIvO2nwZoldT"
      },
      "source": [
        "## Import Data\n",
        "\n",
        "Python supports a number of standard and custom libraries to read files of all types into python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nM2C9QsGoldU",
        "outputId": "696f6856-e053-47f9-b54d-924ca97544d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-06 13:40:08--  https://raw.githubusercontent.com/dearbharat/NLP/main/Course-Description.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 640 [text/plain]\n",
            "Saving to: ‘./Course-Description.txt’\n",
            "\n",
            "./Course-Descriptio 100%[===================>]     640  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-06 13:40:08 (22.9 MB/s) - ‘./Course-Description.txt’ saved [640/640]\n",
            "\n",
            "Data read from file :  In order to construct data pipelines and networks that stream, process, and store data, data engineers and data-science DevOps specialists must understand how to combine multiple big data technologies\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/dearbharat/NLP/main/Course-Description.txt'\n",
        "\n",
        "! wget https://raw.githubusercontent.com/dearbharat/NLP/main/Course-Description.txt -O ./Course-Description.txt\n",
        "\n",
        "\n",
        "#Read the file using standard python libaries\n",
        "with open(os.getcwd()+ \"/Course-Description.txt\", 'r') as fh:  \n",
        "    filedata = fh.read()\n",
        "    \n",
        "#Print first 200 characters in the file\n",
        "print(\"Data read from file : \", filedata[0:200] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02pC746IoldW"
      },
      "source": [
        "## Reading using NLTK CorpusReader\n",
        "\n",
        "Read the same text file using a Corpus Reader\n",
        "\n",
        "NLTK supports multiple CorpusReaders depending upon the type of data source. Details available in http://www.nltk.org/howto/corpus.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2riMvOYOoldW",
        "outputId": "b4c22336-5e6b-4f73-ffef-dd8d2b1e22a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In order to construct data pipelines and networks that stream, process, and store data, data engineers and data-science DevOps specialists must understand how to combine multiple big data technologies. In this course, discover how to build big data pipelines around Apache Spark. Join Kumaran Ponnambalam as he takes you through how to make Apache Spark work with other big data technologies. He covers the basics of Apache Kafka Connect and how to integrate it with Spark for real-time streaming. In addition, he demonstrates how to use the various technologies to construct an end-to-end project that solves a real-world business problem.\n"
          ]
        }
      ],
      "source": [
        "#install nltk using \"pip install nltk\"\n",
        "import nltk\n",
        "#Download punkt package, used part of the other commands\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "\n",
        "#Read the file into a corpus. The same command can read an entire directory\n",
        "corpus=PlaintextCorpusReader(os.getcwd(),\"Course-Description.txt\")\n",
        "\n",
        "#Print raw contents of the corpus\n",
        "print(corpus.raw())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMDJ6pFYoldX"
      },
      "source": [
        "## Exploring the Corpus\n",
        "\n",
        "The corpus library supports a number of functions to extract words, paragraphs and sentences from the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0A2EbFG_oldX",
        "outputId": "bd8d357a-f94f-4207-819d-7e7018b638f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in this corpus :  ['Course-Description.txt']\n",
            "\n",
            " Total paragraphs in this corpus :  1\n",
            "\n",
            " Total sentences in this corpus :  5\n",
            "\n",
            " The first sentence :  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and', 'data', '-', 'science', 'DevOps', 'specialists', 'must', 'understand', 'how', 'to', 'combine', 'multiple', 'big', 'data', 'technologies', '.']\n",
            "\n",
            " The last sentence :  ['In', 'addition', ',', 'he', 'demonstrates', 'how', 'to', 'use', 'the', 'various', 'technologies', 'to', 'construct', 'an', 'end', '-', 'to', '-', 'end', 'project', 'that', 'solves', 'a', 'real', '-', 'world', 'business', 'problem', '.']\n",
            "\n",
            " Words in this corpus :  ['In', 'order', 'to', 'construct', 'data', 'pipelines', ...]\n"
          ]
        }
      ],
      "source": [
        "#Extract the file IDs from the corpus\n",
        "print(\"Files in this corpus : \", corpus.fileids())\n",
        "\n",
        "#Extract paragraphs from the corpus\n",
        "paragraphs=corpus.paras()\n",
        "print(\"\\n Total paragraphs in this corpus : \", len(paragraphs))\n",
        "\n",
        "#Extract sentences from the corpus\n",
        "sentences=corpus.sents()\n",
        "print(\"\\n Total sentences in this corpus : \", len(sentences))\n",
        "print(\"\\n The first sentence : \", sentences[0])\n",
        "print(\"\\n The last sentence : \", sentences[-1])\n",
        "\n",
        "#Extract words from the corpus\n",
        "print(\"\\n Words in this corpus : \",corpus.words() )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMcPv7b0oldY"
      },
      "source": [
        "## Analyze the Corpus\n",
        "\n",
        "The NLTK library provides a number of functions to analyze the distributions and aggregates for data in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO2R9Ms1oldY",
        "outputId": "80836152-b3da-47f5-a618-a7ddcaa65253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words in the corpus :  [('to', 8), ('data', 7), (',', 5), ('-', 5), ('how', 5), ('.', 5), ('and', 4), ('In', 3), ('big', 3), ('technologies', 3)]\n",
            "\n",
            " Distribution for \"Spark\" :  3\n"
          ]
        }
      ],
      "source": [
        "#Find the frequency distribution of words in the corpus\n",
        "course_freq_dist=nltk.FreqDist(corpus.words())\n",
        "\n",
        "#Print most commonly used words\n",
        "print(\"Top 10 words in the corpus : \", course_freq_dist.most_common(10))\n",
        "\n",
        "#find the distribution for a specific word\n",
        "print(\"\\n Distribution for \\\"Spark\\\" : \",course_freq_dist.get(\"Spark\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleansing and Extraction\n",
        "## Tokenization\n",
        "\n",
        "Tokenization refers to converting a text string into individual tokens. Tokens may be words or punctations"
      ],
      "metadata": {
        "id": "cjFeDiS4qOp3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHGgtWmeoldZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec949ce-34a6-4d60-d764-c721ffd716b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token List :  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and']\n",
            "\n",
            " Total Tokens :  110\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import os\n",
        "\n",
        "\n",
        "#Read the base file into a raw text variable\n",
        "base_file = open(os.getcwd()+ \"/Course-Description.txt\", 'rt')\n",
        "raw_text = base_file.read()\n",
        "base_file.close()\n",
        "\n",
        "#Extract tokens\n",
        "token_list = nltk.word_tokenize(raw_text)\n",
        "print(\"Token List : \",token_list[:20])\n",
        "print(\"\\n Total Tokens : \",len(token_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleansing Text\n",
        "\n",
        "We will see examples of removing punctuation and converting to lower case\n",
        "\n",
        "#### Remove Punctuation"
      ],
      "metadata": {
        "id": "qMfEHSXdqWUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the Punkt library to extract tokens\n",
        "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
        "print(\"Token List after removing punctuation : \",token_list2[:20])\n",
        "print(\"\\nTotal tokens after removing punctuation : \", len(token_list2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6neACkpTqXGJ",
        "outputId": "155935a7-ef9a-4fe2-d54d-7ad090a3b147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token List after removing punctuation :  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', 'process', 'and', 'store', 'data', 'data', 'engineers', 'and', 'data-science', 'DevOps', 'specialists']\n",
            "\n",
            "Total tokens after removing punctuation :  100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert to Lower Case"
      ],
      "metadata": {
        "id": "oa-5HskzqaTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_list3=[word.lower() for word in token_list2 ]\n",
        "print(\"Token list after converting to lower case : \", token_list3[:20])\n",
        "print(\"\\nTotal tokens after converting to lower case : \", len(token_list3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEsDoTx5qeoc",
        "outputId": "1a747fab-7a09-485f-896d-f7b8c0bacc7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token list after converting to lower case :  ['in', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', 'process', 'and', 'store', 'data', 'data', 'engineers', 'and', 'data-science', 'devops', 'specialists']\n",
            "\n",
            "Total tokens after converting to lower case :  100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop word Removal\n",
        "\n",
        "Removing stop words by using a standard stop word list available in NLTK for English"
      ],
      "metadata": {
        "id": "Zp7mzJEhqiEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the standard stopword list\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#Remove stopwords\n",
        "token_list4 = list(filter(lambda token: token not in stopwords.words('english'), token_list3))\n",
        "print(\"Token list after removing stop words : \", token_list4[:20])\n",
        "print(\"\\nTotal tokens after removing stop words : \", len(token_list4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tagpXLWlqilh",
        "outputId": "e15176b4-df90-4eee-8359-03b2a0c090d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token list after removing stop words :  ['order', 'construct', 'data', 'pipelines', 'networks', 'stream', 'process', 'store', 'data', 'data', 'engineers', 'data-science', 'devops', 'specialists', 'must', 'understand', 'combine', 'multiple', 'big', 'data']\n",
            "\n",
            "Total tokens after removing stop words :  62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "A4aVxAztqjUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the PorterStemmer library for stemming.\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#Stem data\n",
        "token_list5 = [stemmer.stem(word) for word in token_list4 ]\n",
        "print(\"Token list after stemming : \", token_list5[:20])\n",
        "print(\"\\nTotal tokens after Stemming : \", len(token_list5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRZMkgBoqjuf",
        "outputId": "e0cdeabb-9176-47a4-d970-d9c108cb300b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token list after stemming :  ['order', 'construct', 'data', 'pipelin', 'network', 'stream', 'process', 'store', 'data', 'data', 'engin', 'data-sci', 'devop', 'specialist', 'must', 'understand', 'combin', 'multipl', 'big', 'data']\n",
            "\n",
            "Total tokens after Stemming :  62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "t1Fjmi-6qkEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the wordnet library to map words to their lemmatized form\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4 ]\n",
        "print(\"Token list after Lemmatization : \", token_list6[:20])\n",
        "print(\"\\nTotal tokens after Lemmatization : \", len(token_list6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGTlxTVKqkoS",
        "outputId": "64fc5f15-1344-4492-97b9-79e10aa22a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token list after Lemmatization :  ['order', 'construct', 'data', 'pipeline', 'network', 'stream', 'process', 'store', 'data', 'data', 'engineer', 'data-science', 'devops', 'specialist', 'must', 'understand', 'combine', 'multiple', 'big', 'data']\n",
            "\n",
            "Total tokens after Lemmatization :  62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparison of tokens between raw, stemming and lemmatization"
      ],
      "metadata": {
        "id": "_sM7ZvzAq2b7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for token technlogies\n",
        "print( \"Raw : \", token_list4[20],\" , Stemmed : \", token_list5[20], \" , Lemmatized : \", token_list6[20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyLfLX8Vq25b",
        "outputId": "ac003c83-ba23-4a48-9fda-946ff4db8b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw :  technologies  , Stemmed :  technolog  , Lemmatized :  technology\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Text Processing"
      ],
      "metadata": {
        "id": "Nqwcw5njrMKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare data\n",
        "\n",
        "import nltk\n",
        "import os\n",
        "#Download punkt package, used part of the other commands\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Read the base file into a token list\n",
        "base_file = open(os.getcwd()+ \"/Course-Description.txt\", 'rt')\n",
        "raw_text = base_file.read()\n",
        "base_file.close()\n",
        "\n",
        "#Execute the same pre-processing done in module 3\n",
        "token_list = nltk.word_tokenize(raw_text)\n",
        "\n",
        "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
        "\n",
        "token_list3=[word.lower() for word in token_list2 ]\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "token_list4 = list(filter(lambda token: token not in stopwords.words('english'), token_list3))\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4 ]\n",
        "\n",
        "print(\"\\n Total Tokens : \",len(token_list6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h13cSsrrLbs",
        "outputId": "6d63e8db-656b-4914-b9d2-a20faabcb944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Total Tokens :  62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build ngrams"
      ],
      "metadata": {
        "id": "25KK84iHrUZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "#Find bigrams and print the most common 5\n",
        "bigrams = ngrams(token_list6,2)\n",
        "print(\"Most common bigrams : \")\n",
        "print(Counter(bigrams).most_common(5))\n",
        "\n",
        "#Find trigrams and print the most common 5\n",
        "trigrams = ngrams(token_list6,3)\n",
        "print(\" \\n Most common trigrams : \" )\n",
        "print(Counter(trigrams).most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvyCiTV6rVV5",
        "outputId": "551e30e7-659f-4a7d-a7cb-54603ffefb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common bigrams : \n",
            "[(('big', 'data'), 3), (('data', 'pipeline'), 2), (('data', 'technology'), 2), (('apache', 'spark'), 2), (('order', 'construct'), 1)]\n",
            " \n",
            " Most common trigrams : \n",
            "[(('big', 'data', 'technology'), 2), (('order', 'construct', 'data'), 1), (('construct', 'data', 'pipeline'), 1), (('data', 'pipeline', 'network'), 1), (('pipeline', 'network', 'stream'), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parts-of-Speech Tagging\n",
        "\n",
        "Some examples of Parts-of-Speech abbreviations:\n",
        "NN : noun\n",
        "NNS : noun plural\n",
        "VBP : Verb singular present."
      ],
      "metadata": {
        "id": "y766TsnPrYpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download the tagger package\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#Tag and print the first 10 tokens\n",
        "nltk.pos_tag(token_list4)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEREi9S7rcXQ",
        "outputId": "a84352f8-49b8-4d98-a226-da990598a233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('order', 'NN'),\n",
              " ('construct', 'NN'),\n",
              " ('data', 'NNS'),\n",
              " ('pipelines', 'NNS'),\n",
              " ('networks', 'NNS'),\n",
              " ('stream', 'VBP'),\n",
              " ('process', 'NN'),\n",
              " ('store', 'NN'),\n",
              " ('data', 'NNS'),\n",
              " ('data', 'NNS')]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building TF-IDF matrix"
      ],
      "metadata": {
        "id": "egqaxkflriEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use scikit-learn library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "#Use a small corpus for each visualization\n",
        "vector_corpus = [\n",
        "    'NBA is a Basketball league',\n",
        "    'Basketball is popular in America.',\n",
        "    'TV in America telecast BasketBall.',\n",
        "]\n",
        "\n",
        "#Create a vectorizer for english language\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "#Create the vector\n",
        "tfidf=vectorizer.fit_transform(vector_corpus)\n",
        "\n",
        "print(\"Tokens used as features are : \")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\n Size of array. Each row represents a document. Each column represents a feature/token\")\n",
        "print(tfidf.shape)\n",
        "\n",
        "print(\"\\n Actual TF-IDF array\")\n",
        "tfidf.toarray()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7bfGPuhri_e",
        "outputId": "80df2a4b-e0f9-4cad-c48d-4e1583736d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens used as features are : \n",
            "['america' 'basketball' 'league' 'nba' 'popular' 'telecast' 'tv']\n",
            "\n",
            " Size of array. Each row represents a document. Each column represents a feature/token\n",
            "(3, 7)\n",
            "\n",
            " Actual TF-IDF array\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.38537163, 0.65249088, 0.65249088, 0.        ,\n",
              "        0.        , 0.        ],\n",
              "       [0.54783215, 0.42544054, 0.        , 0.        , 0.72033345,\n",
              "        0.        , 0.        ],\n",
              "       [0.44451431, 0.34520502, 0.        , 0.        , 0.        ,\n",
              "        0.5844829 , 0.5844829 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}